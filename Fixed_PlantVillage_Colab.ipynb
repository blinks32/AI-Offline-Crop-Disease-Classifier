{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üå± PlantVillage Crop Disease Classifier for Android\n",
    "\n",
    "This notebook creates a TensorFlow Lite model from the PlantVillage dataset for your Android app.\n",
    "\n",
    "## üöÄ Quick Start:\n",
    "1. Upload your PlantVillage dataset zip file\n",
    "2. Run all cells\n",
    "3. Download the generated .tflite file\n",
    "4. Replace model.tflite in your Android project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import required packages\n",
    "!pip install tensorflow matplotlib pillow\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from google.colab import files\n",
    "\n",
    "print(\"‚úÖ TensorFlow version:\", tf.__version__)\n",
    "print(\"‚úÖ GPU available:\", len(tf.config.list_physical_devices('GPU')) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'IMAGE_SIZE': (128, 128),  # Match your Android app\n",
    "    'BATCH_SIZE': 32,\n",
    "    'EPOCHS': 15,\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'NUM_CLASSES': 2,  # healthy, diseased\n",
    "    'MODEL_NAME': 'plantvillage_crop_classifier'\n",
    "}\n",
    "\n",
    "print(\"üìã Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload and extract PlantVillage dataset\n",
    "print(\"üìÅ Please upload your PlantVillage dataset zip file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract the dataset\n",
    "zip_filename = list(uploaded.keys())[0]\n",
    "print(f\"\\nüì¶ Extracting {zip_filename}...\")\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')\n",
    "\n",
    "print(\"‚úÖ Extraction completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and explore the dataset structure\n",
    "def find_dataset_folder():\n",
    "    \"\"\"Find the PlantVillage dataset folder\"\"\"\n",
    "    current_dir = Path('.')\n",
    "    \n",
    "    # Look for folders that might contain the dataset\n",
    "    candidates = []\n",
    "    \n",
    "    for item in current_dir.iterdir():\n",
    "        if item.is_dir():\n",
    "            # Count subdirectories\n",
    "            subdirs = [d for d in item.iterdir() if d.is_dir()]\n",
    "            \n",
    "            # Check if it looks like PlantVillage (many class directories)\n",
    "            if len(subdirs) > 10:\n",
    "                # Check for typical PlantVillage class names\n",
    "                subdir_names = [d.name.lower() for d in subdirs]\n",
    "                plant_keywords = ['apple', 'corn', 'tomato', 'potato', 'grape', 'pepper']\n",
    "                health_keywords = ['healthy', 'disease', 'scab', 'rust', 'blight', 'spot']\n",
    "                \n",
    "                has_plants = any(any(plant in name for plant in plant_keywords) for name in subdir_names)\n",
    "                has_health = any(any(health in name for health in health_keywords) for name in subdir_names)\n",
    "                \n",
    "                if has_plants and has_health:\n",
    "                    candidates.append((item, len(subdirs)))\n",
    "    \n",
    "    if candidates:\n",
    "        # Return the folder with the most subdirectories\n",
    "        best_candidate = max(candidates, key=lambda x: x[1])\n",
    "        return best_candidate[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Find the dataset\n",
    "dataset_folder = find_dataset_folder()\n",
    "\n",
    "if dataset_folder:\n",
    "    print(f\"‚úÖ Found dataset folder: {dataset_folder.name}\")\n",
    "    \n",
    "    # List some classes\n",
    "    class_dirs = [d for d in dataset_folder.iterdir() if d.is_dir()]\n",
    "    print(f\"üìä Found {len(class_dirs)} classes\")\n",
    "    print(\"üìã Sample classes:\")\n",
    "    for i, class_dir in enumerate(sorted(class_dirs)[:10]):\n",
    "        image_count = len([f for f in class_dir.iterdir() if f.suffix.lower() in ['.jpg', '.jpeg', '.png']])\n",
    "        print(f\"   {class_dir.name}: {image_count} images\")\n",
    "    \n",
    "    if len(class_dirs) > 10:\n",
    "        print(f\"   ... and {len(class_dirs) - 10} more classes\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Could not find PlantVillage dataset!\")\n",
    "    print(\"Available folders:\")\n",
    "    for item in Path('.').iterdir():\n",
    "        if item.is_dir():\n",
    "            subdirs = len([d for d in item.iterdir() if d.is_dir()])\n",
    "            print(f\"   üìÅ {item.name} ({subdirs} subdirectories)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize PlantVillage for binary classification (healthy vs diseased)\n",
    "def reorganize_plantvillage(source_folder):\n",
    "    \"\"\"Reorganize PlantVillage dataset into healthy/diseased binary classification\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Reorganizing PlantVillage for binary classification...\")\n",
    "    \n",
    "    # Create output directories\n",
    "    output_dir = Path(\"crop_disease_dataset\")\n",
    "    train_dir = output_dir / \"train\"\n",
    "    val_dir = output_dir / \"val\"\n",
    "    \n",
    "    # Remove existing output directory if it exists\n",
    "    if output_dir.exists():\n",
    "        shutil.rmtree(output_dir)\n",
    "    \n",
    "    # Create directory structure\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        for class_name in [\"healthy\", \"diseased\"]:\n",
    "            (output_dir / split / class_name).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Process each class directory\n",
    "    source_path = Path(source_folder)\n",
    "    class_dirs = [d for d in source_path.iterdir() if d.is_dir()]\n",
    "    \n",
    "    healthy_total = 0\n",
    "    diseased_total = 0\n",
    "    processed_classes = 0\n",
    "    \n",
    "    for class_dir in class_dirs:\n",
    "        class_name = class_dir.name.lower()\n",
    "        \n",
    "        # Determine if healthy or diseased\n",
    "        if \"healthy\" in class_name:\n",
    "            target_class = \"healthy\"\n",
    "        else:\n",
    "            target_class = \"diseased\"\n",
    "        \n",
    "        # Find all image files (multiple extensions)\n",
    "        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.JPG', '.JPEG', '.PNG']\n",
    "        image_files = []\n",
    "        \n",
    "        for ext in image_extensions:\n",
    "            image_files.extend(list(class_dir.glob(f\"*{ext}\")))\n",
    "        \n",
    "        if len(image_files) > 0:\n",
    "            print(f\"   üìÅ {class_dir.name}: {len(image_files)} images ‚Üí {target_class}\")\n",
    "            \n",
    "            # Shuffle images for random split\n",
    "            np.random.shuffle(image_files)\n",
    "            \n",
    "            # Split 80/20 train/validation\n",
    "            split_idx = int(len(image_files) * 0.8)\n",
    "            train_files = image_files[:split_idx]\n",
    "            val_files = image_files[split_idx:]\n",
    "            \n",
    "            # Copy training files\n",
    "            for i, img_file in enumerate(train_files):\n",
    "                dest = train_dir / target_class / f\"{class_dir.name}_{i:04d}.jpg\"\n",
    "                try:\n",
    "                    shutil.copy2(img_file, dest)\n",
    "                except Exception as e:\n",
    "                    print(f\"     ‚ö†Ô∏è Error copying {img_file.name}: {e}\")\n",
    "            \n",
    "            # Copy validation files\n",
    "            for i, img_file in enumerate(val_files):\n",
    "                dest = val_dir / target_class / f\"{class_dir.name}_{i:04d}.jpg\"\n",
    "                try:\n",
    "                    shutil.copy2(img_file, dest)\n",
    "                except Exception as e:\n",
    "                    print(f\"     ‚ö†Ô∏è Error copying {img_file.name}: {e}\")\n",
    "            \n",
    "            # Update counters\n",
    "            if target_class == \"healthy\":\n",
    "                healthy_total += len(image_files)\n",
    "            else:\n",
    "                diseased_total += len(image_files)\n",
    "            \n",
    "            processed_classes += 1\n",
    "    \n",
    "    # Verify the reorganization\n",
    "    train_healthy = len(list((train_dir / \"healthy\").glob(\"*\")))\n",
    "    train_diseased = len(list((train_dir / \"diseased\").glob(\"*\")))\n",
    "    val_healthy = len(list((val_dir / \"healthy\").glob(\"*\")))\n",
    "    val_diseased = len(list((val_dir / \"diseased\").glob(\"*\")))\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset reorganization completed!\")\n",
    "    print(f\"üìä Processed {processed_classes} classes\")\n",
    "    print(f\"üìä Training set: {train_healthy} healthy, {train_diseased} diseased\")\n",
    "    print(f\"üìä Validation set: {val_healthy} healthy, {val_diseased} diseased\")\n",
    "    print(f\"üìä Total images: {train_healthy + train_diseased + val_healthy + val_diseased}\")\n",
    "    \n",
    "    if train_healthy + train_diseased + val_healthy + val_diseased == 0:\n",
    "        print(\"‚ùå No images were processed! Check the dataset structure.\")\n",
    "        return None, None\n",
    "    \n",
    "    return str(train_dir), str(val_dir)\n",
    "\n",
    "# Reorganize the dataset\n",
    "if dataset_folder:\n",
    "    TRAIN_DIR, VAL_DIR = reorganize_plantvillage(dataset_folder)\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without dataset folder\")\n",
    "    TRAIN_DIR, VAL_DIR = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators (only if we have valid directories)\n",
    "if TRAIN_DIR and VAL_DIR:\n",
    "    # Data augmentation for training\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    # Validation data (no augmentation)\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # Create generators\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        target_size=CONFIG['IMAGE_SIZE'],\n",
    "        batch_size=CONFIG['BATCH_SIZE'],\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        VAL_DIR,\n",
    "        target_size=CONFIG['IMAGE_SIZE'],\n",
    "        batch_size=CONFIG['BATCH_SIZE'],\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Data generators created!\")\n",
    "    print(f\"üìä Training samples: {train_generator.samples}\")\n",
    "    print(f\"üìä Validation samples: {val_generator.samples}\")\n",
    "    print(f\"üìä Classes: {train_generator.class_indices}\")\n",
    "    \n",
    "    # Show sample images\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sample_batch, sample_labels = next(train_generator)\n",
    "    \n",
    "    for i in range(min(8, len(sample_batch))):\n",
    "        plt.subplot(2, 4, i + 1)\n",
    "        plt.imshow(sample_batch[i])\n",
    "        class_name = 'Healthy' if sample_labels[i][0] > 0.5 else 'Diseased'\n",
    "        plt.title(f'{class_name}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Training Images')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot create data generators without valid dataset directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MobileNetV2 model (optimized for mobile)\n",
    "def create_mobile_model(num_classes=2, input_shape=(128, 128, 3)):\n",
    "    # Use MobileNetV2 as base (mobile-optimized)\n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=input_shape,\n",
    "        alpha=0.75,  # Width multiplier for smaller model\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # Freeze base model initially\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Add custom classification head\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_mobile_model(\n",
    "    num_classes=CONFIG['NUM_CLASSES'],\n",
    "    input_shape=(*CONFIG['IMAGE_SIZE'], 3)\n",
    ")\n",
    "\n",
    "print(\"üèóÔ∏è Model created!\")\n",
    "print(f\"üìä Total parameters: {model.count_params():,}\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model\n",
    "if TRAIN_DIR and VAL_DIR:\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=CONFIG['LEARNING_RATE']),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3,\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"üöÄ Starting training...\")\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=CONFIG['EPOCHS'],\n",
    "        validation_data=val_generator,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Training completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot train without valid dataset\")\n",
    "    # Create a dummy trained model for demonstration\n",
    "    print(\"üîÑ Creating demo model with initialized weights...\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Dummy training data\n",
    "    dummy_x = np.random.random((10, *CONFIG['IMAGE_SIZE'], 3)).astype(np.float32)\n",
    "    dummy_y = tf.keras.utils.to_categorical(\n",
    "        np.random.randint(0, CONFIG['NUM_CLASSES'], 10), \n",
    "        CONFIG['NUM_CLASSES']\n",
    "    )\n",
    "    \n",
    "    model.fit(dummy_x, dummy_y, epochs=1, verbose=0)\n",
    "    print(\"‚úÖ Demo model ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history (if we have real training)\n",
    "if 'history' in locals():\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final metrics\n",
    "    final_acc = history.history['val_accuracy'][-1]\n",
    "    print(f\"üéØ Final validation accuracy: {final_acc:.3f}\")\n",
    "else:\n",
    "    print(\"üìä No training history to display (demo mode)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow Lite with INT8 quantization\n",
    "def convert_to_tflite_quantized(model, model_name):\n",
    "    # Representative dataset for quantization\n",
    "    def representative_dataset():\n",
    "        for _ in range(100):\n",
    "            # Generate representative data\n",
    "            data = np.random.random((1, *CONFIG['IMAGE_SIZE'], 3)).astype(np.float32)\n",
    "            yield [data]\n",
    "    \n",
    "    # Try INT8 quantization first (best for mobile)\n",
    "    print(\"üîÑ Converting to TensorFlow Lite with INT8 quantization...\")\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_dataset\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8\n",
    "    converter.inference_output_type = tf.int8\n",
    "    \n",
    "    try:\n",
    "        tflite_model = converter.convert()\n",
    "        filename = f'{model_name}_int8.tflite'\n",
    "        quantized = True\n",
    "        print(\"‚úÖ INT8 quantized model created!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è INT8 quantization failed: {e}\")\n",
    "        print(\"üîÑ Falling back to float32...\")\n",
    "        \n",
    "        # Fallback to float32\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        tflite_model = converter.convert()\n",
    "        filename = f'{model_name}_float32.tflite'\n",
    "        quantized = False\n",
    "        print(\"‚úÖ Float32 model created!\")\n",
    "    \n",
    "    # Save the model\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    print(f\"üíæ Model saved as: {filename}\")\n",
    "    print(f\"üìä Model size: {len(tflite_model)} bytes ({len(tflite_model)/1024:.1f} KB)\")\n",
    "    \n",
    "    return tflite_model, filename\n",
    "\n",
    "# Convert the model\n",
    "tflite_model, tflite_filename = convert_to_tflite_quantized(model, CONFIG['MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the TensorFlow Lite model\n",
    "def test_tflite_model(filename):\n",
    "    print(f\"üß™ Testing TensorFlow Lite model: {filename}\")\n",
    "    \n",
    "    # Load the model\n",
    "    interpreter = tf.lite.Interpreter(model_path=filename)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    # Get input and output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    print(\"\\nüîç Model Details:\")\n",
    "    print(f\"   Input shape: {input_details[0]['shape']}\")\n",
    "    print(f\"   Input type: {input_details[0]['dtype']}\")\n",
    "    print(f\"   Output shape: {output_details[0]['shape']}\")\n",
    "    print(f\"   Output type: {output_details[0]['dtype']}\")\n",
    "    \n",
    "    # Test with sample data\n",
    "    input_shape = input_details[0]['shape']\n",
    "    input_dtype = input_details[0]['dtype']\n",
    "    \n",
    "    if input_dtype == np.int8:\n",
    "        test_input = np.random.randint(-128, 127, input_shape, dtype=np.int8)\n",
    "        print(\"   Using INT8 input range: [-128, 127]\")\n",
    "    elif input_dtype == np.uint8:\n",
    "        test_input = np.random.randint(0, 255, input_shape, dtype=np.uint8)\n",
    "        print(\"   Using UINT8 input range: [0, 255]\")\n",
    "    else:\n",
    "        test_input = np.random.random(input_shape).astype(np.float32)\n",
    "        print(\"   Using FLOAT32 input range: [0, 1]\")\n",
    "    \n",
    "    # Run inference\n",
    "    interpreter.set_tensor(input_details[0]['index'], test_input)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    print(f\"\\nüß™ Test Results:\")\n",
    "    print(f\"   Raw output: {output}\")\n",
    "    \n",
    "    # Convert output based on type\n",
    "    if output_details[0]['dtype'] == np.int8:\n",
    "        # Convert int8 to probabilities\n",
    "        probs = (output.astype(np.float32) + 128) / 255.0\n",
    "        print(f\"   Converted probabilities: {probs}\")\n",
    "        predicted_class = np.argmax(probs)\n",
    "        confidence = np.max(probs)\n",
    "    else:\n",
    "        predicted_class = np.argmax(output)\n",
    "        confidence = np.max(output)\n",
    "    \n",
    "    class_names = ['healthy', 'diseased']\n",
    "    print(f\"   Predicted: {class_names[predicted_class]} ({confidence:.3f} confidence)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Test the model\n",
    "test_success = test_tflite_model(tflite_filename)\n",
    "print(\"\\n‚úÖ Model testing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels.txt file for Android\n",
    "labels = ['healthy', 'diseased']\n",
    "with open('labels.txt', 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write(f'{label}\\n')\n",
    "\n",
    "print(\"üìù labels.txt created!\")\n",
    "print(f\"üìã Labels: {labels}\")\n",
    "\n",
    "# Display final file information\n",
    "model_size = os.path.getsize(tflite_filename)\n",
    "labels_size = os.path.getsize('labels.txt')\n",
    "\n",
    "print(f\"\\nüìä Generated Files:\")\n",
    "print(f\"   üì± {tflite_filename}: {model_size:,} bytes ({model_size/1024:.1f} KB)\")\n",
    "print(f\"   üìù labels.txt: {labels_size} bytes\")\n",
    "\n",
    "print(f\"\\nüéØ Model Specifications:\")\n",
    "print(f\"   üìê Input: {CONFIG['IMAGE_SIZE'][0]}x{CONFIG['IMAGE_SIZE'][1]}x3\")\n",
    "print(f\"   üè∑Ô∏è Classes: {CONFIG['NUM_CLASSES']} (healthy, diseased)\")\n",
    "print(f\"   üî¢ Type: {'INT8 Quantized' if 'int8' in tflite_filename else 'Float32'}\")\n",
    "print(f\"   üíæ Size: {model_size/1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files for Android integration\n",
    "print(\"üì± Ready for Android Integration!\")\n",
    "print(\"\\nüîΩ Downloading files...\")\n",
    "\n",
    "# Download the model and labels\n",
    "files.download(tflite_filename)\n",
    "files.download('labels.txt')\n",
    "\n",
    "print(\"\\n‚úÖ Files downloaded successfully!\")\n",
    "print(\"\\nüìã Next Steps:\")\n",
    "print(\"1. üìÅ Replace 'app/src/main/assets/model.tflite' with the downloaded model\")\n",
    "print(\"2. üìù Replace 'app/src/main/assets/labels.txt' with the downloaded labels\")\n",
    "print(\"3. üî® Build your Android project: ./gradlew assembleDebug\")\n",
    "print(\"4. üì± Install and test your app\")\n",
    "print(\"5. üéâ Your app should now detect crop diseases in real-time!\")\n",
    "\n",
    "print(\"\\nüí° Tips:\")\n",
    "print(\"- The model expects 128x128 pixel images\")\n",
    "print(\"- Input should be INT8 format (range -128 to 127)\")\n",
    "print(\"- Output is INT8 format (convert to probabilities)\")\n",
    "print(\"- Your Android code is already configured for this model format!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "file_mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}